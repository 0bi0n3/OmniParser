{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up models...\n",
      "Models loaded successfully on 'cuda'.\n",
      "Starting batch processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames:   0%|          | 0/2850 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 111 icons, 14.2ms\n",
      "Speed: 3.2ms preprocess, 14.2ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(filtered_boxes): 119 68\n",
      "time to get parsed content: 0.14050030708312988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames:   0%|          | 1/2850 [00:05<4:03:11,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 110 icons, 13.7ms\n",
      "Speed: 3.2ms preprocess, 13.7ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(filtered_boxes): 113 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames:   0%|          | 2/2850 [00:10<4:01:58,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to get parsed content: 0.25359582901000977\n",
      "\n",
      "0: 736x1280 76 icons, 13.7ms\n",
      "Speed: 3.2ms preprocess, 13.7ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(filtered_boxes): 81 43\n",
      "time to get parsed content: 0.19688200950622559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames:   0%|          | 3/2850 [00:12<2:56:47,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 172 icons, 13.7ms\n",
      "Speed: 3.3ms preprocess, 13.7ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(filtered_boxes): 168 80\n",
      "time to get parsed content: 0.2469627857208252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames:   0%|          | 4/2850 [00:19<3:58:00,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 188 icons, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(filtered_boxes): 188 72\n",
      "time to get parsed content: 0.276660680770874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames:   0%|          | 5/2850 [00:28<5:10:20,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 154 icons, 13.7ms\n",
      "Speed: 3.3ms preprocess, 13.7ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(filtered_boxes): 163 78\n",
      "time to get parsed content: 0.18999862670898438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames:   0%|          | 6/2850 [00:33<4:51:35,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 52 icons, 13.7ms\n",
      "Speed: 3.3ms preprocess, 13.7ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(filtered_boxes): 58 41\n",
      "time to get parsed content: 0.10229063034057617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames:   0%|          | 7/2850 [00:39<4:44:48,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 117 icons, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(filtered_boxes): 134 83\n",
      "time to get parsed content: 0.20371484756469727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames:   0%|          | 8/2850 [00:47<5:06:28,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 145 icons, 13.7ms\n",
      "Speed: 26.4ms preprocess, 13.7ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(filtered_boxes): 149 44\n",
      "time to get parsed content: 0.2633230686187744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames:   0%|          | 9/2850 [01:05<8:06:24, 10.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 150 icons, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(filtered_boxes): 157 51\n",
      "time to get parsed content: 0.31902098655700684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames:   0%|          | 10/2850 [01:19<8:52:12, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 189 icons, 13.7ms\n",
      "Speed: 3.3ms preprocess, 13.7ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(filtered_boxes): 197 99\n",
      "time to get parsed content: 0.25031065940856934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames:   0%|          | 11/2850 [01:29<8:35:20, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 196 icons, 13.7ms\n",
      "Speed: 3.3ms preprocess, 13.7ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(filtered_boxes): 206 80\n",
      "time to get parsed content: 0.2833876609802246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames:   0%|          | 12/2850 [01:36<7:38:24,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 108 icons, 13.7ms\n",
      "Speed: 3.3ms preprocess, 13.7ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(filtered_boxes): 125 64\n",
      "time to get parsed content: 0.17770123481750488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames:   0%|          | 13/2850 [01:40<6:26:14,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x1280 121 icons, 13.7ms\n",
      "Speed: 3.2ms preprocess, 13.7ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 1280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(filtered_boxes): 122 59\n",
      "time to get parsed content: 0.15809226036071777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames:   0%|          | 14/2850 [01:44<5:14:38,  6.66s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import shutil\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm # A library for creating smart progress bars\n",
    "\n",
    "# --- Make sure your utility functions are importable ---\n",
    "from util.utils import get_som_labeled_img, check_ocr_box, get_caption_model_processor, get_yolo_model\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. SETUP - This is the same setup code you had before\n",
    "# ==============================================================================\n",
    "print(\"Setting up models...\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load Detection Model (YOLO)\n",
    "detection_model_path = 'weights/icon_detect/model.pt'\n",
    "som_model = get_yolo_model(detection_model_path)\n",
    "som_model.to(device)\n",
    "\n",
    "# Load Caption Model (Florence-2)\n",
    "caption_model_processor = get_caption_model_processor(\n",
    "    model_name=\"florence2\",\n",
    "    model_name_or_path=\"weights/icon_caption_florence\",\n",
    "    device=device\n",
    ")\n",
    "print(f\"Models loaded successfully on '{device}'.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CONFIGURATION - Define your input/output folders and settings\n",
    "# ==============================================================================\n",
    "INPUT_FRAMES_DIR = \"input_frames\"         # Folder containing your extracted video frames\n",
    "OUTPUT_IMAGE_DIR = \"outputs/images\" # Folder to save annotated images\n",
    "OUTPUT_JSON_DIR = \"outputs/json\"    # Folder to save the structured data\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(OUTPUT_IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_JSON_DIR, exist_ok=True)\n",
    "\n",
    "# Model inference settings\n",
    "BOX_TRESHOLD = 0.05\n",
    "# Note: box_overlay_ratio is now calculated inside the loop for each image\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. BATCH PROCESSING - The main loop\n",
    "# ==============================================================================\n",
    "print(\"Starting batch processing...\")\n",
    "# Get a list of all image files in the input directory\n",
    "image_files = [f for f in os.listdir(INPUT_FRAMES_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Loop through each file with a tqdm progress bar\n",
    "for filename in tqdm(image_files, desc=\"Processing Frames\"):\n",
    "    try:\n",
    "        base_filename = os.path.splitext(filename)[0]\n",
    "        input_image_path = os.path.join(INPUT_FRAMES_DIR, filename)\n",
    "        \n",
    "        # Define where the output files for this frame will go\n",
    "        output_image_path = os.path.join(OUTPUT_IMAGE_DIR, f\"{base_filename}_annotated.jpg\")\n",
    "        output_json_path = os.path.join(OUTPUT_JSON_DIR, f\"{base_filename}_data.json\")\n",
    "\n",
    "        # --- RESUME LOGIC ---\n",
    "        # If the JSON file already exists, skip this frame\n",
    "        if os.path.exists(output_json_path):\n",
    "            continue\n",
    "\n",
    "        # --- DYNAMIC BBOX CONFIG ---\n",
    "        # Open the image to calculate the dynamic overlay ratio\n",
    "        with Image.open(input_image_path) as temp_img:\n",
    "            box_overlay_ratio = max(temp_img.size) / 3200\n",
    "        \n",
    "        draw_bbox_config = {\n",
    "            'text_scale': 0.8 * box_overlay_ratio,\n",
    "            'text_thickness': max(int(2 * box_overlay_ratio), 1),\n",
    "            'text_padding': max(int(3 * box_overlay_ratio), 1),\n",
    "            'thickness': max(int(3 * box_overlay_ratio), 1),\n",
    "        }\n",
    "\n",
    "        # --- OCR PASS ---\n",
    "        ocr_bbox_rslt, _ = check_ocr_box(\n",
    "            input_image_path,\n",
    "            display_img=False,\n",
    "            output_bb_format='xyxy',\n",
    "            use_paddleocr=True,\n",
    "            easyocr_args={'paragraph': False, 'text_threshold': 0.9}\n",
    "        )\n",
    "        text, ocr_bbox = ocr_bbox_rslt if ocr_bbox_rslt is not None else ([], [])\n",
    "\n",
    "        # --- MAIN PARSING ---\n",
    "        # Ensure the function returns a value before unpacking\n",
    "        result = get_som_labeled_img(\n",
    "            input_image_path,\n",
    "            som_model,\n",
    "            BOX_TRESHOLD=BOX_TRESHOLD,\n",
    "            output_coord_in_ratio=True,\n",
    "            ocr_bbox=ocr_bbox,\n",
    "            draw_bbox_config=draw_bbox_config,\n",
    "            caption_model_processor=caption_model_processor,\n",
    "            ocr_text=text,\n",
    "            use_local_semantics=True,\n",
    "            iou_threshold=0.7,\n",
    "            scale_img=False,\n",
    "            batch_size=128\n",
    "        )\n",
    "        \n",
    "        if result is None:\n",
    "            print(f\"Skipping {filename}: No elements found.\")\n",
    "            continue\n",
    "            \n",
    "        annotated_image, label_coordinates, parsed_content_list = result\n",
    "\n",
    "        # --- SAVE THE RESULTS ---\n",
    "        # Move the annotated image from its temporary location\n",
    "        image = Image.open(io.BytesIO(base64.b64decode(annotated_image)))\n",
    "\n",
    "        with open(output_image_path, 'w') as f:\n",
    "            image.save(output_image_path)\n",
    "\n",
    "        # Save the structured data to a JSON file\n",
    "        with open(output_json_path, 'w') as f:\n",
    "            json.dump(parsed_content_list, f, indent=4)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {filename}. Error: {e}\")\n",
    "        # Continue to the next file even if one fails\n",
    "        continue\n",
    "\n",
    "print(\"Batch processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not run below this line!\n",
    "Old code below, only for reference.\n",
    "\n",
    "Refer to 'demo.ipynb' for full original script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from util.utils import get_som_labeled_img, check_ocr_box, get_caption_model_processor, get_yolo_model\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('Setting up models...')\n",
    "device = 'cuda'\n",
    "\n",
    "detection_model_path='weights/icon_detect/model.pt' # YOLO\n",
    "som_model = get_yolo_model(detection_model_path)\n",
    "som_model.to(device)\n",
    "\n",
    "print('model to {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load caption model\n",
    "caption_model_processor = get_caption_model_processor(\n",
    "    model_name='florence2',\n",
    "    model_name_or_path=\"weights/icon_caption_florence\",\n",
    "    device=device\n",
    ")\n",
    "print(f'Models loaded successfully on {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and check input/output directories exist\n",
    "INPUT_FRAMES_DIR = 'input_frames'\n",
    "OUTPUT_IMAGE_DIR = 'outputs/images'\n",
    "OUTPUT_JSON_DIR = 'outputs/json'\n",
    "\n",
    "os.makedirs(OUTPUT_IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_JSON_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model inference settings\n",
    "BOX_TRESHOLD = 0.05\n",
    "draw_bbox_config = {\n",
    "    'text_scale': 0.8,\n",
    "    'text_thickness': 1,\n",
    "    'text_padding': 1,\n",
    "    'thickness': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils\n",
    "import shutil\n",
    "importlib.reload(utils)\n",
    "\n",
    "print(\"Starting batch processing...\")\n",
    "\n",
    "image_files = [f for f in os.listdir(INPUT_FRAMES_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "for filename in tqdm(image_files, desc=\"Processing Frames\"):\n",
    "    try:\n",
    "        base_filename = os.path.splitext(filename)[0]\n",
    "        input_image_path = os.path.join(INPUT_FRAMES_DIR, filename)\n",
    "\n",
    "        output_image_path = os.path.join(OUTPUT_IMAGE_DIR, f\"{base_filename}_annotated.png\")\n",
    "        output_json_path = os.path.join(OUTPUT_JSON_DIR, f\"{base_filename}_data.json\")\n",
    "\n",
    "        if os.path.exists(output_json_path):\n",
    "            continue\n",
    "        \n",
    "        # ocr_bbox_result, _ = check_ocr_box(\n",
    "        #     input_image_path,\n",
    "        #     display_img=False,\n",
    "        #     output_bb_format='xyxy',\n",
    "        #     use_paddleocr=True,\n",
    "        # )\n",
    "        ocr_bbox = [ ]\n",
    "        text = [ ]\n",
    "\n",
    "        annotated_image, label_coordinates, parsed_content_list = get_som_labeled_img(\n",
    "            input_image_path,\n",
    "            som_model,\n",
    "            BOX_TRESHOLD=BOX_TRESHOLD,\n",
    "            output_coord_in_ratio=True,\n",
    "            ocr_bbox=ocr_bbox,\n",
    "            draw_bbox_config=draw_bbox_config,\n",
    "            caption_model_processor=caption_model_processor,\n",
    "            ocr_text=text,\n",
    "            use_local_semantics=True,\n",
    "            iou_threshold=0.7,\n",
    "            scale_img=False,\n",
    "            batch_size=128,\n",
    "        )\n",
    "        \n",
    "        #save_image = Image.open(annotated_image)\n",
    "        #save_image.save(output_image_path)\n",
    "\n",
    "        shutil.move(annotated_image, output_image_path)\n",
    "\n",
    "        with open(output_json_path, 'w') as f:\n",
    "            json.dump(parsed_content_list, f, indent=4)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {filename}. Error: {e}\")\n",
    "        continue\n",
    "\n",
    "print('Batch processing complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two choices for caption model: fine-tuned blip2 or florence2\n",
    "import importlib\n",
    "# import util.utils\n",
    "# importlib.reload(utils)\n",
    "from util.utils import get_som_labeled_img, check_ocr_box, get_caption_model_processor, get_yolo_model\n",
    "caption_model_processor = get_caption_model_processor(model_name=\"florence2\", model_name_or_path=\"weights/icon_caption_florence\", device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som_model.device, type(som_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload utils\n",
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "# from utils import get_som_labeled_img, check_ocr_box, get_caption_model_processor, get_yolo_model\n",
    "\n",
    "# image_path = 'imgs/google_page.png'\n",
    "# image_path = 'imgs/windows_home.png'\n",
    "# image_path = 'imgs/windows_multitab.png'\n",
    "# image_path = 'imgs/omni3.jpg'\n",
    "# image_path = 'imgs/ios.png'\n",
    "# image_path = 'imgs/word.png'\n",
    "# image_path = 'imgs/excel2.png'\n",
    "# image_path = '/home/oberon/projects/OmniParser/imgs/frame_022500_game_1.jpg'\n",
    "# image_path = '/home/oberon/projects/OmniParser/imgs/frame_023100_game_1.jpg'\n",
    "# image_path = 'imgs/frame_025200_game_1.jpg'\n",
    "image_path = 'imgs/frame_027300_game_1.jpg'\n",
    "\n",
    "\n",
    "image = Image.open(image_path)\n",
    "image_rgb = image.convert('RGB')\n",
    "print('image size:', image.size)\n",
    "\n",
    "box_overlay_ratio = max(image.size) / 3200\n",
    "draw_bbox_config = {\n",
    "    'text_scale': 0.8 * box_overlay_ratio,\n",
    "    'text_thickness': max(int(2 * box_overlay_ratio), 1),\n",
    "    'text_padding': max(int(3 * box_overlay_ratio), 1),\n",
    "    'thickness': max(int(3 * box_overlay_ratio), 1),\n",
    "}\n",
    "BOX_TRESHOLD = 0.05\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "ocr_bbox_rslt, is_goal_filtered = check_ocr_box(image_path, display_img = False, output_bb_format='xyxy', goal_filtering=None, easyocr_args={'paragraph': False, 'text_threshold':0.9}, use_paddleocr=True)\n",
    "text, ocr_bbox = ocr_bbox_rslt\n",
    "cur_time_ocr = time.time() \n",
    "\n",
    "dino_labled_img, label_coordinates, parsed_content_list = get_som_labeled_img(image_path, som_model, BOX_TRESHOLD = BOX_TRESHOLD, output_coord_in_ratio=True, ocr_bbox=ocr_bbox,draw_bbox_config=draw_bbox_config, caption_model_processor=caption_model_processor, ocr_text=text,use_local_semantics=True, iou_threshold=0.7, scale_img=False, batch_size=128)\n",
    "cur_time_caption = time.time() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dino_labled_img it is in base64\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "image = Image.open(io.BytesIO(base64.b64decode(dino_labled_img)))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.imshow(image)\n",
    "# print(len(parsed_content_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(parsed_content_list)\n",
    "df['ID'] = range(len(df))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('UE5_menu_viewport_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
